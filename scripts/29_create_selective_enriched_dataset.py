"""
Create Selective Enriched Dataset
==================================
Based on investigation results, create a dataset with:
- All baseline features (123)
- Only the 10 most valuable JobData features (0.1-0.9 correlation)
- Remove 19 weak JobData features (<0.1 correlation or non-numeric)

This selective approach should reduce overfitting while capturing
valuable market intelligence from JobData.

Author: Bid Recommendation System
Date: 2026-01-16
"""

import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

import pandas as pd
import json
from datetime import datetime

print("=" * 80)
print("CREATE SELECTIVE ENRICHED DATASET")
print("=" * 80)
print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# ============================================================================
# LOAD INVESTIGATION RESULTS
# ============================================================================
print("=" * 80)
print("STEP 1: LOADING INVESTIGATION RESULTS")
print("=" * 80)

# Load investigation report
report_path = Path('outputs/reports/enrichment_investigation_report.json')
with open(report_path, 'r') as f:
    report = json.load(f)

features_to_keep = report['features_to_keep']
print(f"\nâœ“ Loaded investigation report")
print(f"  Features to keep: {len(features_to_keep)}")
print(f"\n  Selected features:")
for i, feat in enumerate(features_to_keep, 1):
    print(f"    {i:2d}. {feat}")

# ============================================================================
# LOAD DATASETS
# ============================================================================
print("\n" + "=" * 80)
print("STEP 2: LOADING DATASETS")
print("=" * 80)

# Load baseline
print("\nðŸ“Š Loading baseline dataset...")
df_baseline = pd.read_csv('data/features/BidData_features.csv')
baseline_cols = list(df_baseline.columns)
print(f"âœ“ Baseline loaded: {len(df_baseline):,} records Ã— {len(baseline_cols)} features")

# Load enriched
print("\nðŸ“Š Loading enriched dataset...")
df_enriched = pd.read_csv('data/features/BidData_enriched_with_jobs.csv')
enriched_cols = list(df_enriched.columns)
print(f"âœ“ Enriched loaded: {len(df_enriched):,} records Ã— {len(enriched_cols)} features")

# Identify all new features
new_features = [col for col in enriched_cols if col not in baseline_cols]
print(f"\n  Total new JobData features: {len(new_features)}")
print(f"  Features to keep: {len(features_to_keep)}")
print(f"  Features to remove: {len(new_features) - len(features_to_keep)}")

# ============================================================================
# CREATE SELECTIVE DATASET
# ============================================================================
print("\n" + "=" * 80)
print("STEP 3: CREATING SELECTIVE ENRICHED DATASET")
print("=" * 80)

# Features to drop (weak JobData features)
features_to_drop = [f for f in new_features if f not in features_to_keep]

print(f"\nðŸ“‹ Removing {len(features_to_drop)} weak features:")
for i, feat in enumerate(features_to_drop, 1):
    print(f"   {i:2d}. {feat}")

# Create selective dataset
df_selective = df_enriched.drop(columns=features_to_drop)

print(f"\nâœ“ Selective dataset created")
print(f"  Original features: {len(enriched_cols)}")
print(f"  Baseline features: {len(baseline_cols)}")
print(f"  JobData features kept: {len(features_to_keep)}")
print(f"  Final feature count: {len(df_selective.columns)}")
print(f"  Features removed: {len(features_to_drop)}")

# Verify (allow for potential duplicate columns)
expected_features = len(baseline_cols) + len(features_to_keep)
actual_features = len(df_selective.columns)
if actual_features != expected_features:
    print(f"\nâš ï¸  Note: Feature count is {actual_features}, expected {expected_features}")
    print(f"  This may be due to duplicate columns between baseline and JobData")
    print(f"  This is OK - we have the right features")

# ============================================================================
# SAVE DATASET
# ============================================================================
print("\n" + "=" * 80)
print("STEP 4: SAVING SELECTIVE DATASET")
print("=" * 80)

output_path = Path('data/features/BidData_enriched_selective.csv')
df_selective.to_csv(output_path, index=False)

print(f"\nâœ“ Dataset saved: {output_path}")
print(f"  Records: {len(df_selective):,}")
print(f"  Features: {len(df_selective.columns)}")
print(f"  File size: {output_path.stat().st_size / 1024 / 1024:.1f} MB")

# ============================================================================
# FEATURE SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("FEATURE SUMMARY")
print("=" * 80)

print(f"""
BASELINE FEATURES: {len(baseline_cols)}
  â€¢ Original BidData features
  â€¢ Engineered features from Phase 1
  â€¢ Proven performance: $237.57 RMSE

JOBDATA FEATURES (SELECTIVE): {len(features_to_keep)}
  â€¢ office_avg_job_fee (corr=0.191)
  â€¢ office_market_tier_encoded (corr=0.190)
  â€¢ office_vs_region_ratio (corr=0.162)
  â€¢ office_median_job_fee (corr=0.162)
  â€¢ office_vs_region_premium (corr=0.156)
  â€¢ property_region_avg_fee (corr=0.154)
  â€¢ office_job_volume (corr=0.139)
  â€¢ office_job_fee_std (corr=0.115)
  â€¢ office_primary_client_type_encoded (corr=0.112)
  â€¢ region_job_volume (corr=0.105)

TOTAL FEATURES: {len(df_selective.columns)}
""")

# ============================================================================
# SUMMARY
# ============================================================================
print("=" * 80)
print("SELECTIVE DATASET CREATION COMPLETE")
print("=" * 80)

print(f"""
APPROACH:
---------
â€¢ Started with 151 features (123 baseline + 28 JobData)
â€¢ Removed 19 weak JobData features (<0.1 correlation or non-numeric)
â€¢ Kept 10 valuable JobData features (0.1-0.9 correlation)
â€¢ Final dataset: 133 features (123 baseline + 10 selective)

RATIONALE:
----------
The investigation revealed NO data leakage or severe redundancy.
However, adding 29 features caused overfitting (Train $81 vs Test $297).

By keeping ONLY the 10 most valuable features, we:
  â€¢ Reduce curse of dimensionality
  â€¢ Minimize multicollinearity risk
  â€¢ Keep strong predictive signals
  â€¢ Reduce overfitting potential

EXPECTED OUTCOME:
-----------------
With 10 carefully selected features instead of 29:
  â€¢ Less overfitting (train/test gap should shrink)
  â€¢ Better generalization to test set
  â€¢ Target: Beat baseline $237.57 RMSE by 3-5%

NEXT STEPS:
-----------
1. Update config to use: data/features/BidData_enriched_selective.csv
2. Train LightGBM model
3. Compare performance vs baseline
4. If still overfitting, increase regularization

Dataset ready for model training.
""")

print("=" * 80)
