# Session 2026-02-17

## Objective
Implement model improvements identified from backtest results. Systematic improvement cycle: implement one change at a time, validate, commit.

## Improvements Implemented

### 1. Filter Anomaly Fees (< $100) from Training
- Added `MIN_TRAINING_FEE = 100` to `config/model_config.py`
- Applied filter in `scripts/24_enhanced_bidfee_model.py` during data loading
- Removed 322 rows out of 215K (pro-bono/internal work with $1 fees)
- **Results**: MAPE 11.3% → 4.3%, overfitting 2.09x → 1.91x, Value Dispute MAPE 47.7% → 4.7%

### 2. Isotonic Regression Calibration for Win Probability
- Added calibration step to `scripts/25_enhanced_win_probability.py`
- Fits `IsotonicRegression` on validation set predictions vs actuals
- Saves calibrator as `win_probability_v2_calibrator.pkl`
- API loads and applies calibrator in `_predict_win_probability()`
- **Results**: 60-80% calibration gap -11.3% → -0.1%, Brier 0.0938 → 0.0927

### 3. Sample-Size Confidence Warnings
- API returns `warnings` array when segment/state has < 100 training samples
- Frontend displays amber warning banner with specific low-data messages
- Graceful UX for rare combos (Non Real Estate, Transaction Services, etc.)

### 4. Model Drift Monitoring
- Created `scripts/29_model_drift_monitor.py`
- Monthly rolling MAPE/AUC tracking with configurable alert thresholds
- KS-test for feature distribution shift between train and test
- **Results**: All clear — no drift detected across any month

### 5. Time-Series Cross-Validation
- Created `scripts/30_cross_validation.py`
- 5-fold expanding-window CV for both models
- **Results**: Bid fee MAPE 4.8% +/- 1.0%, Win prob AUC 0.9597 +/- 0.0026
- Extremely stable across all folds — confirms robust generalization

### 6. Ensemble Blending for Rare Combos
- When segment/state has < 100 samples, blend prediction with segment average
- Blend weight scales linearly: fewer samples = more reliance on segment avg
- Prevents wild predictions for unseen segment/state combinations

### 7. Consolidated Validation Summary
- Created `scripts/31_validation_summary.py`
- Single report covering: training metrics, overfitting, backtest, CV, drift, calibration
- **Verdict**: ALL CHECKS PASSED

## Final Model Performance

| Metric | Before (Session 16) | After (Session 17) | Change |
|--------|---------------------|---------------------|--------|
| Bid Fee MAPE | 11.3% | 4.3% | -62% |
| Bid Fee Overfit | 2.09x | 1.91x | Improved |
| Win Prob Brier | 0.0938 | 0.0927 | -1.2% |
| 60-80% Calibration Gap | -11.3% | -0.1% | Fixed |
| Confident Accuracy | 94.7% | 95.5% | +0.8% |
| CV Bid Fee MAPE | N/A | 4.8% +/- 1.0% | New |
| CV Win Prob AUC | N/A | 0.9597 +/- 0.0026 | New |

## Files Created
- `scripts/29_model_drift_monitor.py`
- `scripts/30_cross_validation.py`
- `scripts/31_validation_summary.py`
- `outputs/reports/drift_monitor_report.json`
- `outputs/reports/cross_validation_results.json`
- `outputs/models/win_probability_v2_calibrator.pkl`

## Files Modified
- `config/model_config.py` — added MIN_TRAINING_FEE
- `scripts/24_enhanced_bidfee_model.py` — anomaly fee filter
- `scripts/25_enhanced_win_probability.py` — isotonic calibration
- `scripts/28_model_backtest.py` — calibrator support, bug fixes
- `api/enhanced_prediction_service.py` — calibrator, warnings, blending
- `frontend/src/components/ResultDisplay.js` — warning display
- `frontend/src/components/ResultDisplay.css` — warning styles
