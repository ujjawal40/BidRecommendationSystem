# Session 2026-02-17

## Objective
Implement model improvements identified from backtest results. Systematic improvement cycle: implement one change at a time, validate, commit.

## Improvements Implemented

### 1. Filter Anomaly Fees (< $100) from Training
- Added `MIN_TRAINING_FEE = 100` to `config/model_config.py`
- Applied filter in `scripts/24_enhanced_bidfee_model.py` during data loading
- Removed 322 rows out of 215K (pro-bono/internal work with $1 fees)
- **Results**: MAPE 11.3% → 4.3%, overfitting 2.09x → 1.91x, Value Dispute MAPE 47.7% → 4.7%

### 2. Isotonic Regression Calibration for Win Probability
- Added calibration step to `scripts/25_enhanced_win_probability.py`
- Fits `IsotonicRegression` on validation set predictions vs actuals
- Saves calibrator as `win_probability_v2_calibrator.pkl`
- API loads and applies calibrator in `_predict_win_probability()`
- **Results**: 60-80% calibration gap -11.3% → -0.1%, Brier 0.0938 → 0.0927

### 3. Sample-Size Confidence Warnings
- API returns `warnings` array when segment/state has < 100 training samples
- Frontend displays amber warning banner with specific low-data messages
- Graceful UX for rare combos (Non Real Estate, Transaction Services, etc.)

### 4. Model Drift Monitoring
- Created `scripts/29_model_drift_monitor.py`
- Monthly rolling MAPE/AUC tracking with configurable alert thresholds
- KS-test for feature distribution shift between train and test
- **Results**: All clear — no drift detected across any month

### 5. Time-Series Cross-Validation
- Created `scripts/30_cross_validation.py`
- 5-fold expanding-window CV for both models
- **Results**: Bid fee MAPE 4.8% +/- 1.0%, Win prob AUC 0.9597 +/- 0.0026
- Extremely stable across all folds — confirms robust generalization

### 6. Ensemble Blending for Rare Combos
- When segment/state has < 100 samples, blend prediction with segment average
- Blend weight scales linearly: fewer samples = more reliance on segment avg
- Prevents wild predictions for unseen segment/state combinations

### 7. Consolidated Validation Summary
- Created `scripts/31_validation_summary.py`
- Single report covering: training metrics, overfitting, backtest, CV, drift, calibration
- **Verdict**: ALL CHECKS PASSED

### 8. Overfitting Reduction via Regularization Experiments
- Created `scripts/32_overfitting_experiments.py` — tested 17 hyperparameter configs
- Each experiment modified one or more parameters from baseline, trained fresh, measured overfit ratio + MAPE
- Three configs hit both targets (overfit <= 1.5x AND MAPE < 6%):
  - `very_strong_reg`: reg_alpha/lambda 20.0 → overfit 1.48x, MAPE 4.8%
  - `min_gain_01`: min_split_gain 0.1 → overfit 1.47x, MAPE 5.4%
  - `combo_balanced`: multiple changes → overfit 1.37x, MAPE 5.6%
- **Selected `very_strong_reg`**: simplest change (one parameter pair), best MAPE among winners
- Applied: `reg_alpha` and `reg_lambda` 5.0 → 20.0 in `scripts/24_enhanced_bidfee_model.py`
- Retrained production model
- **Results**: Overfit 1.91x → 1.49x, Test MAPE 3.9%, R² 0.9403
- Backtest confirms: MAPE 4.8%, Within 20%: 96.0%, Median error $26

Key experiment findings:
- `path_smooth` had minimal effect on overfitting (1.84x with value 10, 1.81x with value 50)
- `min_split_gain` was very effective but aggressive values (0.5) hurt MAPE too much
- `combo_max_regularize` hit 1.15x overfit but MAPE jumped to 7.2% — over-regularized
- No single technique alone could reach 1.5x target; `very_strong_reg` (reg=20) was the only single-change winner

## Final Model Performance

| Metric | Start of Session | After Improvements | After Regularization |
|--------|-----------------|--------------------|--------------------|
| Bid Fee MAPE (backtest) | 11.3% | 4.3% | **4.8%** |
| Bid Fee Overfit | 2.09x | 1.91x | **1.49x** |
| Bid Fee RMSE | $490 | $420 | **$439** |
| Win Prob Brier | 0.0938 | 0.0927 | 0.0927 |
| 60-80% Calibration Gap | -11.3% | -0.1% | -0.1% |
| Confident Accuracy | 94.7% | 95.5% | 95.5% |
| CV Bid Fee MAPE | N/A | 4.8% +/- 1.0% | 4.8% +/- 1.0% |
| CV Win Prob AUC | N/A | 0.9597 +/- 0.0026 | 0.9597 +/- 0.0026 |

## Files Created
- `scripts/29_model_drift_monitor.py`
- `scripts/30_cross_validation.py`
- `scripts/31_validation_summary.py`
- `scripts/32_overfitting_experiments.py`
- `outputs/reports/drift_monitor_report.json`
- `outputs/reports/cross_validation_results.json`
- `outputs/reports/overfitting_experiments.json`
- `outputs/models/win_probability_v2_calibrator.pkl`

## Files Modified
- `config/model_config.py` — added MIN_TRAINING_FEE
- `scripts/24_enhanced_bidfee_model.py` — anomaly fee filter, reg_alpha/lambda 5.0 → 20.0
- `scripts/25_enhanced_win_probability.py` — isotonic calibration
- `scripts/28_model_backtest.py` — calibrator support, bug fixes
- `api/enhanced_prediction_service.py` — calibrator, warnings, blending
- `frontend/src/components/ResultDisplay.js` — warning display
- `frontend/src/components/ResultDisplay.css` — warning styles

## Commits (24 total)
- 20 commits for improvements 1-7 + session notes
- 4 commits for overfitting experiment + regularization retrain
