# Session Log — 2026-02-11

## Prompt 1: Resume from yesterday's session
**User**: Continue from where last session stopped (context was compacted).
**Action**: Read session summary, identified pending work: bid fee experiment script needed to be run (failed yesterday due to wrong working directory).

## Prompt 2: Run bid fee experiments (Phase A)
**Action**: Ran `scripts/50_bidfee_model_experiments.py` with 9 configs.
**Results**:
- A1 baseline: RMSE $354, MAPE 71.2%, overfit 1.99x (CAP at 500)
- A7 (LR 0.01 + 2500): RMSE $345, MAPE 70.8%, overfit 1.99x — RMSE winner
- A9 (log + LR 0.02 + 1500): RMSE $531, MAPE 15.7%, overfit 1.60x — MAPE winner
- All 9 experiments hit their round cap (no early stopping triggered)

## Prompt 3: Run Phase B experiments (6 more configs)
**Action**: Added B1-B6 experiments combining best strategies from Phase A.
**Results** (15 total):
- B6 (raw + LR 0.01 + 5000): RMSE $329 but 2.92x overfitting — eliminated
- B3 (log + huber + LR 0.01 + 3000): RMSE $468, MAPE 39.2%, 1.50x
- B5 (DART): Completely broken (best_iteration=0, DART has no early stopping)
- No Phase B config beat A9 for MAPE within overfitting constraint

## Prompt 4: Apply A9 to production
**Decision**: A9 (log1p + LR 0.02 + 1500 rounds) as production model.
**Rationale**: MAPE 15.7% vs 71.2% (4.5x better), overfitting 1.60x vs 1.99x.
**Files modified**:
- `config/model_config.py`: learning_rate 0.05→0.02, num_boost_round 500→1500, added target_transform "log1p"
- `scripts/04_model_lightgbm.py`: log1p before training, expm1 after prediction, target_transform in metadata
- `api/prediction_service.py`: expm1 after model.predict(), benchmark-aware floor

**Retrained model**: RMSE $531, MAPE 15.7%, R² 0.9377, overfit 1.60x. All 26 tests pass.

## Prompt 5: Edge case discovery
**Issue**: Delaware/Estate/Hospitality combo predicted $123 (log-transform underprediction on rare combos).
**Fix**: Benchmark-aware floor: `max($500, 30% × segment_avg)` → $978 for this combo.

## Prompt 6: Backdated commits on main
**Action**: Started creating commits on main but user stopped — wants separate branches.

## Prompt 7: Branch-based workflow
**User**: Use separate branches per improvement, not monolithic commits on main.
**Action**: Reset main to `6d6ef90`, created `feature/win-prob-bidfee-retrain` branch.
**Commits so far** (3 of ~7):
1. `ea53d4c` Add LEAKY_JOB_DERIVED_FEATURES to model config (Feb 10 14:00)
2. `e5a4a41` Retrain win probability model with BidFee as feature (Feb 10 14:30)
3. `9daec55` Add blended benchmark and inject BidFee into win prob model (Feb 10 15:00)

## Prompt 8: Split commits + no AI attribution + documentation updates
**User**: Split into 6-7 commits per branch. Never say "Co-Authored-By: Claude" — use user's name. Update CLAUDE.md with branches, architecture, commit conventions. Create sessions file. Update LEARNINGS.md.
**Action**: Updated CLAUDE.md (architecture diagram, model performance, branch map, commit conventions, critical conventions). Updated LEARNINGS.md (win prob retrain results, bid fee experiments, log-transform edge case). Updated MEMORY.md. Created this sessions file.

## Prompt 9: Push branches to remote
**Action**: Pushed both `feature/win-prob-bidfee-retrain` and `feature/bidfee-log-transform` to remote.

## Prompt 10: Merge both branches into main
**Action**: Merged both feature branches into main. No conflicts. Pushed to remote.

## Prompt 11: Deployment questions
**User**: Do I need to redeploy Render and Vercel?
**Answer**: Only Render — frontend hasn't changed.
**User**: How to enable auto-deploy on Render free tier?
**Answer**: Settings → Build & Deploy → Auto-Deploy → Yes.

## Prompt 12: Fix win probability confidence labels
**User**: Screenshots from live app showed 86%/84%/81% win probabilities with "medium" or "low" confidence — looks contradictory.
**Root cause**: Confidence hierarchy capped win prob confidence at bid fee confidence level. Delaware has limited data → low/medium bid fee confidence → caps win prob confidence even when model is 86% certain.
**Fix**: Removed the confidence hierarchy cap. Win prob model now includes BidFee as a direct feature, so its confidence is independent.
**Thresholds** (distance from 0.5):
- High: >0.3 (probability >80% or <20%)
- Medium: 0.15–0.3 (probability 65–80% or 20–35%)
- Low: <0.15 (probability 35–65%)

**Files modified**:
- `api/prediction_service.py`: Removed 6-line cap logic, removed `bid_fee_confidence` parameter from `predict_win_probability()`
- `tests/test_validation.py`: Updated test from `test_win_prob_confidence_capped_by_bid_fee` → `test_win_prob_confidence_independent_of_bid_fee`

**Commit**: `7c67c00` — pushed to main. All 26 tests pass.

## Status
- [x] Both feature branches merged to main
- [x] Win prob confidence cap removed
- [x] All changes pushed to remote
- [ ] Render redeploy needed for changes to go live
