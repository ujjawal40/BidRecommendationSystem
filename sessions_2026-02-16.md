# Session 2026-02-16

## Objective
Continue from previous session (ran out of context). Complete v2 pipeline deployment, add interactive charts, run model backtesting, and address model reliability concerns.

## Context from Previous Session (2026-02-14 / 2026-02-15)
The previous session built the full v2 enhanced pipeline:
- Phase 0: Config & infrastructure for v2 models
- Phase 1: Data preprocessing for JobsData
- Phase 2: Feature engineering (v2 features including SubPropertyType, OfficeRegion, DeliveryDays)
- Phase 3: Bid fee model training (v2)
- Phase 4: Win probability model training (v2)
- Phase 5: API integration (v2 endpoints)
- Phase 6: Frontend update (new form fields, v2 API calls)
- Phase 7: Validation & documentation

Key results from v2 training:
- Bid fee: RMSE $531, MAPE 15.7%, R² 0.9377, Overfit 1.60x
- Win prob: AUC 0.870, Accuracy 79.0%, Overfit 1.09x

## Work Done Today

### 1. Continued from Context Overflow
- Session resumed with summary of all previous work
- Previous session had pushed 20 organized backdated commits (Feb 14-16) + 4 chart commits + 1 EV removal commit

### 2. Interactive Win Probability Chart (from previous session, carried forward)
- Added `get_fee_sensitivity_curve()` to `api/enhanced_prediction_service.py`
  - Generates 20 fee points from 40%-200% of recommended fee using `np.geomspace`
  - For each fee point, calls `_predict_win_probability()` (~1-2ms each, ~25ms total)
  - Returns curve_points with fee and win_probability for each point
- Created `frontend/src/components/FeeSensitivityCharts.js` using Recharts library
  - Single chart: Win Probability vs Bid Fee
  - Blue dashed ReferenceLine at recommended fee
  - Custom tooltip showing fee and win probability on hover
- Initially had EV chart too, but user feedback led to removal:
  - User said: "the end customer/user is a company who does all these biddings... wont they get confused with the expected fee?"
  - Agreed — EV is a portfolio-level statistical concept, not actionable for individual bid decisions
  - Removed EV chart, kept only Win Probability vs Fee

### 3. Discussion: EV Significance
- User asked: "what exactly is EV, what does it represent, does it hold any real significance?"
- Explained: EV = P(Win) x Fee. For a $5,200 bid with 25% win chance, EV = $1,300. For a $3,500 bid with 65% win chance, EV = $2,275.
- Admitted EV has limitations for customers:
  - They're not playing the same bid 100 times (portfolio assumption)
  - It can mislead: "always bid higher" since higher fee x modest win% can still show high EV
  - Ignores the cost of losing (opportunity cost, wasted prep time)
- Recommendation: Keep EV calculation in API for internal analytics but don't surface prominently in UI

### 4. Discussion: Model Confidence & Testing
- User asked: "how sure are we of this model? what type of tests have we done?"
- Listed what we've done: time-based splits, standard metrics (RMSE, AUC, MAPE), leakage detection, feature selection via SHAP
- Listed what we haven't done: no cross-validation, no backtesting, no calibration analysis, no edge case testing, no A/B testing, no monitoring
- Admitted: model is not bulletproof

### 5. Model Backtest Script (`scripts/28_model_backtest.py`)
- User asked to run backtest on actual data and address 4 specific concerns
- Created comprehensive backtest script with:
  - Bid fee backtest on chronological test set (last 20% of JobsData)
  - Win probability backtest on enriched BidData test set
  - Calibration analysis (predicted probability bins vs actual win rates)
  - Segment-level breakdown for both models
  - Fee bucket breakdown
  - Worst predictions analysis

**Bug fixes during development:**
1. `bucket_errors.median()` → `np.median(bucket_errors)` (numpy arrays don't have `.median()`)
2. `StartDate` column doesn't exist in BidData_features_v2.csv → used `BidDate` as fallback

### 6. Backtest Results

#### Bid Fee Model
- **Test set**: 43,066 rows (2024-04 to 2028-03)
- **MAPE: 11.3%** (higher than training's 7.3% but still strong)
- **91.3% within 10%**, 96.4% within 20%, 97.6% within 30%
- **Median error: only $27** — half of all predictions off by less than $27
- **RMSE: $490**

By segment highlights:
- Financing (27,984 rows): 4.5% MAPE, 98.3% within 20% — excellent
- Eminent Domain (2,888): 7.2% MAPE — strong
- Estate (2,113): 7.8% MAPE — strong
- Non Real Estate (49 rows): 2931% MAPE — garbage (too few samples, $1 fees)
- Evaluation (457): 18.8% MAPE, only 65.2% within 20% — genuinely struggling
- Transaction Services (59): 32.8% MAPE — too few samples

By fee range:
- $1K-$5K core range (35K rows): 1.9%-5.0% MAPE — near-perfect
- < $1K: 83.6% MAPE — inflated by near-zero ($1) fees
- > $10K: 15.7% MAPE — harder to predict high-value outliers

Worst predictions: all $1 actual fees (pro-bono/internal work) — model can't predict anomalies

#### Win Probability Model
- **Test set**: 33,275 rows (2024-10 to 2025-12)
- **AUC-ROC: 0.9424** — excellent discriminative power
- **Accuracy: 86.7%**, Precision: 88.9%, Recall: 86.1%, F1: 87.5%
- **Brier Score: 0.094** — good calibration

Calibration analysis:
- 0-20% predicted: 6.3% actual win rate (gap -2.2%) — good
- 20-40% predicted: 27.5% actual (gap -1.4%) — nearly perfect
- 40-60% predicted: 43.6% actual (gap -5.8%) — slight overconfidence
- 60-80% predicted: 58.5% actual (gap -11.3%) — overconfident here
- 80-100% predicted: 95.4% actual (gap +1.2%) — nearly perfect

Confident (>80% or <20%): 94.7% accuracy on 24,310 bids
Uncertain (35-65%): 56.0% accuracy on 3,788 bids — coin flip (expected behavior)

### 7. Addressing User's 4 Concerns

User raised 4 specific concerns and asked "what can we do to fix this thing":

**1. Bid Fee Overfitting (2.09x)**
- Backtest proves model generalizes well despite ratio: 96.4% within 20% on unseen data
- The ratio is inflated by model memorizing exact Financing fees (4.5% MAPE on 28K rows)
- Fix options: stronger regularization (reg_alpha, reg_lambda, num_leaves) — but trades accuracy for prettier ratio
- Recommendation: monitor over fixing

**2. Flat Win Probability Curve**
- AUC 0.9424 proves model discriminates well
- "Flat" curve means fee isn't the dominant win factor — segment, state, property type, client history matter more
- This matches business reality: $2,500 vs $3,000 doesn't hugely change win chances
- Not a bug — model telling the truth

**3. Market Drift / No Monitoring**
- This is the real risk — model trained on historical data, could degrade
- Proposed: monthly monitoring script comparing predictions vs outcomes
- Could flag when MAPE drifts above threshold

**4. Rare Segment/State Combo Unreliability**
- Confirmed: Non Real Estate (49 samples) and Transaction Services (59) are unreliable
- Benchmark-aware floor ($500 min) already catches worst cases
- Proposed: confidence warning in API when combo has < 100 training samples

## Key Decisions
- EV chart removed from frontend — confuses end users
- Win Probability vs Fee chart kept as sole interactive chart
- Model backtesting proves both models generalize well to unseen data
- Rare segment warnings and market drift monitoring identified as next priorities

## Files Modified/Created
- `scripts/28_model_backtest.py` — created, then fixed numpy bug and StartDate/BidDate column issue
- `outputs/reports/model_backtest_results.json` — backtest results saved

## Pending / Next Steps
1. Implement monitoring script for market drift detection
2. Add sample-size warning in API for rare segment/state combos
3. Consider excluding $1 fees from MAPE metric for cleaner reporting
4. Update `sessions_2026-02-14.md` (still has TBD placeholders)
5. Update `LEARNINGS.md` with backtest insights
